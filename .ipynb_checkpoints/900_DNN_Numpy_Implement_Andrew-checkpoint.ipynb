{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation 구현\n",
    "\n",
    "- single hidden layer 의 이진 분류 neural network\n",
    "- cross entropy loss 계산\n",
    "- forward and backward propagation 구현  \n",
    "\n",
    "- [Original source from Coursera - Andrew Ng.](https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb)\n",
    "\n",
    "<img src=\"grad_summary.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z = Wa + b$  \n",
    "$a = sig(Z)$  \n",
    "$L(a, y) = -[y\\cdot log{a} - (1-y)\\cdot log(1-a)]$  \n",
    "\n",
    "\n",
    "- 출력층의 $\\frac{dL}{dZ}$   \n",
    "$\\frac{dL}{da} = -[y\\cdot\\frac{1}{a} - (1-y)\\cdot \\frac{1}{1-a}]$  \n",
    "$\\frac{dL}{dZ} = \\frac{dL}{da}\\frac{da}{dZ} = (-\\frac{y}{a}+\\frac{1-y}{1-a})\\cdot a(1-a) = a - y$\n",
    "\n",
    "\n",
    "- $\\frac{dL}{dW}$  \n",
    "$\\frac{dZ}{dW} = \\frac{d}{dW}(W^{[l]} a^{[l-1]}+b^{[l]}) = a^{[l-1]}$  \n",
    "$\\frac{dL}{dW} = \\frac{dL}{dZ}\\frac{dZ}{dW} = (a^{[l]} - y)\\cdot a^{[l-1]} = \\frac{dL}{dZ}\\cdot a^{[l-1]}$  \n",
    "\n",
    "\n",
    "- $\\frac{dL}{db}$  \n",
    "$\\frac{dZ}{db} = \\frac{d}{db}(W^{[l]} a^{[l-1]}+b^{[l]}) = 1$  \n",
    "$\\frac{dL}{db} = \\frac{dL}{dZ}\\frac{dZ}{db} = \\frac{dL}{dZ}$  \n",
    "\n",
    "\n",
    "- 은닉층의 $\\frac{dL}{dZ}$   \n",
    "$Z^2 = W^2 \\cdot a(Z^1) + b^2$   \n",
    "$\\frac{dZ^2}{dZ^1} = W^2\\cdot g^{1\\prime} (Z^1)$   \n",
    "$\\frac{dL}{dZ^1} = \\frac{dL}{dZ^2}\\frac{dZ^2}{dZ^1} = \\frac{dL}{dZ^2}\\cdot W^2 \\cdot g^{1\\prime} (Z^1)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력층\n",
    "- output A, true value Y  \n",
    "- Loss = binary-crossentropy ($-[ylog{\\hat{y}} + (1-y)log{(1-\\hat{y})}]$)\n",
    "\n",
    "    1. $dZ^{[L]} = A^{[L]}-Y \\rightarrow$ 출력층의 $dZ$\n",
    "    2. $dW^{[L]} = dZ^{[L]}A^{[L-1]} \\rightarrow$ 현재층의 $dW$는 현재층의 $dZ$ 곱하기 이전층의 출력 \n",
    "    3. $db^{[L]} = dZ^{[L]} \\rightarrow$ 현재층의 $db$는 현재층의 $dZ$\n",
    "    \n",
    "### 은닉층\n",
    "-  \n",
    "    4. $dZ^{[L]} = W^{[L+1]}dZ^{[L+1]}\\cdot g^{[L]\\prime} (Z^{[L]}) \\rightarrow$ 현재층의 $dZ$은 다음층의 weight와  $dZ$의 행렬곱 곱하기 현재층의 activation의 미분값\n",
    "    5. $dW^{[L]} = dZ^{[L]}A^{[L-1]} \\rightarrow$ 현재층의 $dW$는 현재층의 $dZ$ 곱하기 이전층의 출력 (이전층이 입력층이면 input data)\n",
    "    6. $db^{[L]} = dZ^{[L]} \\rightarrow$ 현재층의 $db$는 현재층의 $dZ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- logit   \n",
    "    Returns:\n",
    "        A -- output of sigmoid(z), same shape as Z\n",
    "        cache -- backpropagation에 사용하게 위해 Z 저장\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A, Z\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        Z -- logit\n",
    "    Returns:\n",
    "        A -- output of relu, the same shape as Z\n",
    "        cache -- backpropagation에 사용하게 위해 A 저장\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- dL/dA\n",
    "        cache -- 저장된 forward prop. 의 'Z' \n",
    "    Returns:\n",
    "        dZ -- Z에 대한 cost 의 gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ = np.where(Z > 0, dA, 0)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dA -- dL/dA\n",
    "        cache -- 저장된 forward prop. 의 'Z' \n",
    "    Returns:\n",
    "        dZ -- Z에 대한 cost 의 gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현 순서\n",
    "\n",
    "1) 2 layer Network와 $L$ 계층 신경망에 대한 parameter 를 초기화  \n",
    "\n",
    "2) 순방향 전파 모듈을 구현(아래 그림 보라색)\n",
    "- pre-activation(LINEAR) ($Z^{[l]}$)\n",
    "- ACTIVATION (relu / sigmoid)\n",
    "- 두 단계를 새로운 [LINEAR-> ACTIVATION] forward function으로 결합\n",
    "- [LINEAR-> RELU] forward function L-1 회 (layer 1 ~ L-1)을 스택하고 끝에 [LINEAR-> SIGMOID]를 추가 (최종 layer $L$) --> L_model_forward 함수.\n",
    "    \n",
    "3) 손실 계산  \n",
    "\n",
    "\n",
    "4) 역방향 전파 모듈 (아래 그림에서 빨간색으로 표시)을 구현  \n",
    "- 레이어의 역전파 단계의 LINEAR 부분을 완료\n",
    "- ACTIVATE 함수 (relu_backward / sigmoid_backward)의 기울기를 제공\n",
    "- 이전 두 단계를 새로운 [LINEAR-> ACTIVATION] backward function으로 결합\n",
    "- 새 L_model_backward 함수에서 [LINEAR-> RELU]를 뒤로 L-1 번 스택하고 [LINEAR-> SIGMOID]를 뒤로 추가\n",
    "\n",
    "5) 마지막으로 매개 변수를 업데이트  \n",
    "\n",
    "<img src=\"final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) parameter 초기화\n",
    "\n",
    "model의 $L$ 레이어들의 parameter 를 초기화하는 함수를 작성.\n",
    "\n",
    "- random initialization  \n",
    "    - 가중치 행렬에 대해 임의 초기화를 사용 : `np.random.randn(shape)*0.01`\n",
    "    - 초기화 값에 0.01과 같은 작은 스칼라 값를 곱하여 활성화 함수의 derivative 가 0 에 가깝지 않은 영역에 있도록 함  \n",
    "\n",
    "\n",
    "- Xavier initialization  \n",
    "\n",
    "    $$random initialization * \\sqrt{\\frac{1.}{size^{[l-1]}}}$$\n",
    "    \n",
    "    \n",
    "- bias 에 대해 0 초기화 : `np.zeros(shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, xavier=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- 네트워크의 각 계층의 dimension을 포함하는 파이썬 배열(list)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- \"W1\", \"b1\", ..., \"WL\", \"bL\" 매개 변수를 포함하는 파이썬 dictionary:\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(101)\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)     # number of layers in the network\n",
    "\n",
    "    \n",
    "    for layer in range(1, L):\n",
    "        cdim, pdim = layer_dims[layer], layer_dims[layer-1]\n",
    "        if xavier:\n",
    "            parameters['W'+str(layer)] = np.random.randn(cdim, pdim) * np.sqrt(1./cdim)\n",
    "            parameters['b'+str(layer)] = np.zeros((cdim, 1))\n",
    "        else:\n",
    "            parameters['W'+str(layer)] = np.random.randn(cdim, pdim) * 0.01\n",
    "            parameters['b'+str(layer)] = np.zeros((cdim, 1))\n",
    "     \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 784) (16, 64) (1, 16)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters([784, 64, 16, 1]) # 입력 5, 출력 1\n",
    "\n",
    "print(parameters['W1'].shape, parameters['W2'].shape, parameters['W3'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이썬에서 $W X + b$ 를 계산할 때 브로드 캐스팅을 수행한다는 것을 기억. \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "일 경우 $WX + b$ 는:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Forward Propagation\n",
    "\n",
    "### Pre-Activation Forward\n",
    "\n",
    "\n",
    "linear forward 모듈은 다음 방정식을 계산.\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
    "\n",
    "여기서 $A^{[0]} = X$ (입력 data).\n",
    "\n",
    "이전 계층의 입력이 주어지면 각 neuron은 $z = W ^ Tx + b$ 를 계산 한 다음 ReLU와 같은 활성화 함수 g(z)를 적용. 이 과정에서 역전파에 사용될 각 레이어에서 계산되고 사용되는 모든 변수를 저장(캐시). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b, activation_fn):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (number of features, size of previous layer)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    Returns:\n",
    "    Z -- 활성화 함수의 입력, also called pre-activation parameter \n",
    "    cache -- \"A_pre\", \"W\", \"b\" 및 Z를 포함하는 파이썬 dictionary\n",
    "    \"\"\"\n",
    "    # 이전 layer 의 output(A_prev)과 current layer 의 weight 를 곱하여 새로운 logit 생성\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation_fn == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)                # A, Z\n",
    "    elif activation_fn == \"relu\":\n",
    "        A, activation_cache = relu(Z)                   # A, Z\n",
    "    else:\n",
    "        print(\"Invalid activation_fn\")\n",
    "        \n",
    "    # backpropagation 에 사용하기 위해 A_prev, W, b 를 cache 에 저장\n",
    "    cache = ((A_prev, W, b), Z)            #((A_prev, W, b), Z)\n",
    "    # logit 을 activation 함수에 적용한 output 과 cache 를 반환\n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- linear_activation_forward 를 L개 층으로 일반화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    A = X                           \n",
    "    caches = []                     \n",
    "    L = len(parameters) // 2 # w, b 함께 있으므로 2 로 나누어줌    \n",
    "\n",
    "    # 입력층과 은닉층의 activation, W, b, Z 계산\n",
    "    for layer in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_forward(\n",
    "            A_prev, parameters[\"W\" + str(layer)], parameters[\"b\" + str(layer)], activation_fn=\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # 출력층 의 activation, W, b, Z 계산\n",
    "    AL, cache = linear_forward(\n",
    "        A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation_fn=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert AL.shape == (1, X.shape[1])\n",
    "    return AL, caches   # AL: Last Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Loss 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary cross-entropy cost 계산\n",
    "def compute_cost(AL, y):\n",
    "    m = y.shape[1]              \n",
    "    cost = - (1 / m) * np.sum(\n",
    "        np.multiply(y, np.log(AL)) + np.multiply(1 - y, np.log(1 - AL)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation\n",
    "\n",
    "그래디언트를 계산하기 위해 cost 함수로 부터 네트워크를 통해 거꾸로 정보가 되돌아 갈 수 있도록 합니다. 이렇게 하면 누가 가장 많은 오류를 일으키는지 파악하여 그 방향으로 parameter 를 update 할 수 있습니다.  \n",
    "\n",
    "- 출력층 Last activation :  $\\frac{dL}{da} = -[y\\cdot \\frac{1}{a} - (1-y)\\cdot \\frac{1}{1-a}] = -\\frac{y}{a}+\\frac{1-y}{1-a} = -\\frac{y(1-a)}{a(1-a)} - \\frac{a-ay}{a(1-a)} = \\frac{-y+ay+a-ay}{a(1-a)} = \\frac{a-y}{a(1-a)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dA, Z):\n",
    "    A, Z = sigmoid(Z)\n",
    "    dZ = dA * A * (1 - A)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_gradient(dA, Z):\n",
    "    A, Z = relu(Z)\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))  # true 이면 1 else 면 0\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def linear_backward(dA, cache, activation_fn):\n",
    "    \n",
    "    ((A_prev, W, b), Z) = cache\n",
    "\n",
    "    if activation_fn == \"sigmoid\":\n",
    "        dZ = sigmoid_gradient(dA, Z)\n",
    "    elif activation_fn == \"relu\":\n",
    "        dZ = relu_gradient(dA, Z)\n",
    "    else:\n",
    "        print(\"No matching activation function\")\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(AL, y, caches):\n",
    "    y = y.reshape(AL.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "\n",
    "    dAL = np.divide(AL - y, np.multiply(AL, 1 - AL))\n",
    "\n",
    "    # 출력층의 gradient\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] \\\n",
    "         = linear_backward(dAL, caches[L-1], \"sigmoid\")\n",
    "\n",
    "    # 은닉층의 gradient\n",
    "    for layer in range(L - 2, 0, -1):\n",
    "        current_cache = caches[layer]\n",
    "        grads[\"dA\" + str(layer)], grads[\"dW\" + str(layer+1)], grads[\"db\" + str(layer+1)] \\\n",
    "             = linear_backward(grads[\"dA\" + str(layer+1)], current_cache, \"relu\") \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2   # w, b 함께 있으므로 2 로 나누어줌  \n",
    "    for layer in range(1, L):\n",
    "        parameters[\"W\"+str(layer+1)] = parameters[\"W\"+str(layer+1)] - learning_rate * grads[\"dW\"+str(layer+1)]\n",
    "        parameters[\"b\"+str(layer+1)] = parameters[\"b\"+str(layer+1)] - learning_rate * grads[\"db\"+str(layer+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data 준비\n",
    "\n",
    "- mnist data의 2 개 digit 만 선택하여 binary classification 문제로 data 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   779  780  781  782  783  label  \n",
       "0  0.0  0.0  0.0  0.0  0.0      5  \n",
       "1  0.0  0.0  0.0  0.0  0.0      0  \n",
       "2  0.0  0.0  0.0  0.0  0.0      4  \n",
       "3  0.0  0.0  0.0  0.0  0.0      1  \n",
       "4  0.0  0.0  0.0  0.0  0.0      9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"mnist.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    6903\n",
       "5.0    6313\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.where((df['label'] == 5) | (df['label'] == 0)).dropna()\n",
    "X['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9  ...  775  776  777  778  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "34  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    779  780  781  782  783  label  \n",
       "0   0.0  0.0  0.0  0.0  0.0    1.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "11  0.0  0.0  0.0  0.0  0.0    1.0  \n",
       "21  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "34  0.0  0.0  0.0  0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['label'].loc[X['label'] == 5] = 1\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9912, 784), (3304, 784), (9912,), (3304,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:, :-1].values, X.iloc[:, -1].values)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot cat image\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap='gray')\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 9912), (784, 3304), (1, 9912), (1, 3304))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize the data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "y_test = y_test.reshape(1, y_test.shape[0])\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the multi-layer model using all the helper functions we wrote before\n",
    "def L_layer_model(X, y, layers_dims, learning_rate=0.01, num_iterations=100):\n",
    "\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims, xavier=True)\n",
    "\n",
    "    # intialize cost list\n",
    "    cost_list = []\n",
    "\n",
    "    # iterate over num_iterations\n",
    "    for i in range(num_iterations):\n",
    "        # iterate over L-layers to get the final output and the cache\n",
    "        AL, caches = L_model_forward(X, parameters)  # AL: Last Activation\n",
    "        \n",
    "        # compute cost to plot it\n",
    "        cost = compute_cost(AL, y)\n",
    "\n",
    "        # iterate over L-layers backward to get gradients\n",
    "        grads = L_model_backward(AL, y, caches)\n",
    "\n",
    "        # update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # append each 100th cost to the cost list\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"The cost after {i + 1} iterations is: {cost:.4f}\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "\n",
    "    # plot the cost curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cost_list)\n",
    "    plt.xlabel(\"Iterations (per hundreds)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss curve for the learning rate = {learning_rate}\")\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def accuracy(X, parameters, y):\n",
    "    probs, caches = L_model_forward(X, parameters)\n",
    "    labels = (probs >= 0.5) * 1\n",
    "    accuracy = np.mean(labels == y) * 100\n",
    "    return f\"The accuracy rate is: {accuracy:.2f}%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 9912) (1, 9912)\n",
      "The cost after 100 iterations is: 0.3413\n",
      "The cost after 200 iterations is: 0.2444\n",
      "The cost after 300 iterations is: 0.1978\n",
      "The cost after 400 iterations is: 0.1707\n",
      "The cost after 500 iterations is: 0.1533\n",
      "The cost after 600 iterations is: 0.1410\n",
      "The cost after 700 iterations is: 0.1317\n",
      "The cost after 800 iterations is: 0.1244\n",
      "The cost after 900 iterations is: 0.1184\n",
      "The cost after 1000 iterations is: 0.1135\n",
      "The cost after 1100 iterations is: 0.1093\n",
      "The cost after 1200 iterations is: 0.1057\n",
      "The cost after 1300 iterations is: 0.1024\n",
      "The cost after 1400 iterations is: 0.0996\n",
      "The cost after 1500 iterations is: 0.0970\n",
      "The cost after 1600 iterations is: 0.0947\n",
      "The cost after 1700 iterations is: 0.0926\n",
      "The cost after 1800 iterations is: 0.0907\n",
      "The cost after 1900 iterations is: 0.0890\n",
      "The cost after 2000 iterations is: 0.0874\n",
      "The cost after 2100 iterations is: 0.0859\n",
      "The cost after 2200 iterations is: 0.0845\n",
      "The cost after 2300 iterations is: 0.0832\n",
      "The cost after 2400 iterations is: 0.0820\n",
      "The cost after 2500 iterations is: 0.0809\n",
      "The cost after 2600 iterations is: 0.0798\n",
      "The cost after 2700 iterations is: 0.0788\n",
      "The cost after 2800 iterations is: 0.0779\n",
      "The cost after 2900 iterations is: 0.0770\n",
      "The cost after 3000 iterations is: 0.0762\n",
      "The cost after 3100 iterations is: 0.0754\n",
      "The cost after 3200 iterations is: 0.0746\n",
      "The cost after 3300 iterations is: 0.0739\n",
      "The cost after 3400 iterations is: 0.0732\n",
      "The cost after 3500 iterations is: 0.0725\n",
      "The cost after 3600 iterations is: 0.0719\n",
      "The cost after 3700 iterations is: 0.0713\n",
      "The cost after 3800 iterations is: 0.0707\n",
      "The cost after 3900 iterations is: 0.0701\n",
      "The cost after 4000 iterations is: 0.0695\n",
      "The cost after 4100 iterations is: 0.0690\n",
      "The cost after 4200 iterations is: 0.0685\n",
      "The cost after 4300 iterations is: 0.0680\n",
      "The cost after 4400 iterations is: 0.0675\n",
      "The cost after 4500 iterations is: 0.0670\n",
      "The cost after 4600 iterations is: 0.0665\n",
      "The cost after 4700 iterations is: 0.0660\n",
      "The cost after 4800 iterations is: 0.0656\n",
      "The cost after 4900 iterations is: 0.0652\n",
      "The cost after 5000 iterations is: 0.0647\n",
      "The cost after 5100 iterations is: 0.0643\n",
      "The cost after 5200 iterations is: 0.0639\n",
      "The cost after 5300 iterations is: 0.0635\n",
      "The cost after 5400 iterations is: 0.0631\n",
      "The cost after 5500 iterations is: 0.0628\n",
      "The cost after 5600 iterations is: 0.0624\n",
      "The cost after 5700 iterations is: 0.0620\n",
      "The cost after 5800 iterations is: 0.0617\n",
      "The cost after 5900 iterations is: 0.0613\n",
      "The cost after 6000 iterations is: 0.0610\n",
      "The cost after 6100 iterations is: 0.0606\n",
      "The cost after 6200 iterations is: 0.0603\n",
      "The cost after 6300 iterations is: 0.0600\n",
      "The cost after 6400 iterations is: 0.0597\n",
      "The cost after 6500 iterations is: 0.0594\n",
      "The cost after 6600 iterations is: 0.0591\n",
      "The cost after 6700 iterations is: 0.0588\n",
      "The cost after 6800 iterations is: 0.0586\n",
      "The cost after 6900 iterations is: 0.0583\n",
      "The cost after 7000 iterations is: 0.0580\n",
      "The cost after 7100 iterations is: 0.0578\n",
      "The cost after 7200 iterations is: 0.0575\n",
      "The cost after 7300 iterations is: 0.0573\n",
      "The cost after 7400 iterations is: 0.0570\n",
      "The cost after 7500 iterations is: 0.0568\n",
      "The cost after 7600 iterations is: 0.0565\n",
      "The cost after 7700 iterations is: 0.0563\n",
      "The cost after 7800 iterations is: 0.0560\n",
      "The cost after 7900 iterations is: 0.0558\n",
      "The cost after 8000 iterations is: 0.0555\n",
      "The cost after 8100 iterations is: 0.0553\n",
      "The cost after 8200 iterations is: 0.0551\n",
      "The cost after 8300 iterations is: 0.0549\n",
      "The cost after 8400 iterations is: 0.0546\n",
      "The cost after 8500 iterations is: 0.0544\n",
      "The cost after 8600 iterations is: 0.0542\n",
      "The cost after 8700 iterations is: 0.0540\n",
      "The cost after 8800 iterations is: 0.0538\n",
      "The cost after 8900 iterations is: 0.0536\n",
      "The cost after 9000 iterations is: 0.0534\n",
      "The cost after 9100 iterations is: 0.0532\n",
      "The cost after 9200 iterations is: 0.0530\n",
      "The cost after 9300 iterations is: 0.0528\n",
      "The cost after 9400 iterations is: 0.0526\n",
      "The cost after 9500 iterations is: 0.0524\n",
      "The cost after 9600 iterations is: 0.0522\n",
      "The cost after 9700 iterations is: 0.0520\n",
      "The cost after 9800 iterations is: 0.0518\n",
      "The cost after 9900 iterations is: 0.0516\n",
      "The cost after 10000 iterations is: 0.0514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The accuracy rate is: 97.52%.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting layers dims\n",
    "layers_dims = [X_train.shape[0], 64, 16, 1]\n",
    "\n",
    "# NN with relu activation fn\n",
    "parameters = L_layer_model(X_train, y_train, layers_dims, learning_rate=0.01, num_iterations=10000)\n",
    "# Print the accuracy\n",
    "accuracy(X_test, parameters, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 10\n",
    "sample_data = X_test[:, [idx]]\n",
    "y_test[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.97368987e-05]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL, caches = L_model_forward(sample_data, parameters)\n",
    "AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
